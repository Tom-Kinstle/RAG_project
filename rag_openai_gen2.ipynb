{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e0ec4d7",
   "metadata": {},
   "source": [
    "# Proelections RAG System - Enhanced with Chroma and Openai\n",
    "This system extracts text from PDFs, embeds document chunks into vector space using HuggingFace E5, stores/retrieves via Chroma, and optionally generates answers using OpenAI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bd2276",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2341e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import torch\n",
    "import os\n",
    "import glob\n",
    "from typing import List, Dict, Any\n",
    "import PyPDF2\n",
    "import pdfplumber\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c7605c",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a02616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API Configuration\n",
    "OPENAI_API_KEY = \"Open Ai Key Here\"\n",
    "\n",
    "# Initialize OpenAI client (for openai>=1.0.0)\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Local PDF directory\n",
    "PDF_DIRECTORY = r\"C:\\Users\\default.LAPTOP-4HP0OBME\\OneDrive\\Documents\\GitHub\\RAG\\pro_elect\\OneDriv\"\n",
    "\n",
    "# Chroma database directory\n",
    "CHROMA_DB_DIR = \"./chroma_db\"\n",
    "\n",
    "# Chunk configs\n",
    "CHUNK_CONFIGS = {\n",
    "    \"default\": {\"size\": 800, \"overlap\": 200},\n",
    "    \"small\": {\"size\": 400, \"overlap\": 100},\n",
    "    \"large\": {\"size\": 1200, \"overlap\": 300}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50e473d",
   "metadata": {},
   "source": [
    "# Embedding Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d13512d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_embedding_model(silent: bool = False):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if not silent:\n",
    "        print(f\"üñ•Ô∏è Using device: {device}\")\n",
    "    try:\n",
    "        return HuggingFaceEmbeddings(\n",
    "            model_name=\"intfloat/e5-base-v2\",\n",
    "            model_kwargs={\"device\": device},\n",
    "            encode_kwargs={\"normalize_embeddings\": True, \"batch_size\": 32}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading E5: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca193b1",
   "metadata": {},
   "source": [
    "# PDF Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34689c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text from PDF using pdfplumber as primary method, PyPDF2 as fallback.\"\"\"\n",
    "    text = \"\"\n",
    "    \n",
    "    try:\n",
    "        # Try pdfplumber first (better text extraction)\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"pdfplumber failed for {pdf_path}, trying PyPDF2: {e}\")\n",
    "        \n",
    "        # Fallback to PyPDF2\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "        except Exception as e2:\n",
    "            print(f\"PyPDF2 also failed for {pdf_path}: {e2}\")\n",
    "            return \"\"\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def find_and_process_pdfs(directory: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Recursively find all PDF files and extract their text.\"\"\"\n",
    "    pdf_files = []\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory not found: {directory}\")\n",
    "        return pdf_files\n",
    "    \n",
    "    # Recursively find all PDF files\n",
    "    for pdf_path in glob.glob(os.path.join(directory, \"**/*.pdf\"), recursive=True):\n",
    "        print(f\"Processing: {pdf_path}\")\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        \n",
    "        if text:\n",
    "            pdf_files.append({\n",
    "                \"filename\": os.path.basename(pdf_path),\n",
    "                \"full_path\": pdf_path,\n",
    "                \"text\": text\n",
    "            })\n",
    "        else:\n",
    "            print(f\"No text extracted from: {pdf_path}\")\n",
    "    \n",
    "    print(f\"Successfully processed {len(pdf_files)} PDF files\")\n",
    "    return pdf_files\n",
    "\n",
    "# Cell 5: Document Chunking\n",
    "def prepare_documents_enhanced(text: str, source_filename: str, chunk_size: int, chunk_overlap: int) -> List[Document]:\n",
    "    \"\"\"Clean and chunk documents with enhanced metadata.\"\"\"\n",
    "    \n",
    "    if not text.strip():\n",
    "        return []\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n=== \", \"\\nARTICLE \", \"\\nSECTION \", \"\\n\\n\", \"\\n\", \". \", \" \"],\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "\n",
    "    raw_docs = splitter.create_documents([text])\n",
    "    enhanced_docs = []\n",
    "\n",
    "    for i, doc in enumerate(raw_docs):\n",
    "        content = doc.page_content.lower()\n",
    "        metadata = {\n",
    "            \"source_file\": source_filename,\n",
    "            \"chunk_id\": i,\n",
    "            \"chunk_size\": len(doc.page_content),\n",
    "            \"has_voting\": any(t in content for t in [\"vote\", \"ballot\", \"election\", \"poll\", \"voting\", \"electoral\"]),\n",
    "            \"has_proxy\": \"proxy\" in content,\n",
    "            \"has_director\": any(t in content for t in [\"director\", \"board\", \"officer\", \"president\", \"secretary\", \"treasurer\"]),\n",
    "            \"has_quorum\": \"quorum\" in content,\n",
    "            \"has_notice\": any(t in content for t in [\"notice\", \"notification\", \"notify\", \"inform\"]),\n",
    "            \"has_meeting\": any(t in content for t in [\"meeting\", \"assembly\", \"session\", \"gathering\"]),\n",
    "        }\n",
    "        enhanced_docs.append(Document(page_content=doc.page_content, metadata=metadata))\n",
    "\n",
    "    return enhanced_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c6bc65",
   "metadata": {},
   "source": [
    "# Document Processing and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9a52f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_documents_enhanced(text: str, source_filename: str, chunk_size: int, chunk_overlap: int) -> List[Document]:\n",
    "    \"\"\"Clean and chunk documents with enhanced metadata.\"\"\"\n",
    "    \n",
    "    if not text.strip():\n",
    "        return []\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n=== \", \"\\nARTICLE \", \"\\nSECTION \", \"\\n\\n\", \"\\n\", \". \", \" \"],\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "\n",
    "    raw_docs = splitter.create_documents([text])\n",
    "    enhanced_docs = []\n",
    "\n",
    "    for i, doc in enumerate(raw_docs):\n",
    "        content = doc.page_content.lower()\n",
    "        metadata = {\n",
    "            \"source_file\": source_filename,\n",
    "            \"chunk_id\": i,\n",
    "            \"chunk_size\": len(doc.page_content),\n",
    "            \"has_voting\": any(t in content for t in [\"vote\", \"ballot\", \"election\", \"poll\", \"voting\", \"electoral\"]),\n",
    "            \"has_proxy\": \"proxy\" in content,\n",
    "            \"has_director\": any(t in content for t in [\"director\", \"board\", \"officer\", \"president\", \"secretary\", \"treasurer\"]),\n",
    "            \"has_quorum\": \"quorum\" in content,\n",
    "            \"has_notice\": any(t in content for t in [\"notice\", \"notification\", \"notify\", \"inform\"]),\n",
    "            \"has_meeting\": any(t in content for t in [\"meeting\", \"assembly\", \"session\", \"gathering\"]),\n",
    "        }\n",
    "        enhanced_docs.append(Document(page_content=doc.page_content, metadata=metadata))\n",
    "\n",
    "    return enhanced_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080b0690",
   "metadata": {},
   "source": [
    "# Vector Store Initialization (Chroma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e336f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_chroma_db(documents: List[Document], embedding_model, persist_directory: str = CHROMA_DB_DIR):\n",
    "    \"\"\"Initialize Chroma vector store with documents.\"\"\"\n",
    "    \n",
    "    if not documents:\n",
    "        print(\"No documents to index\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Create Chroma vector store\n",
    "        vector_store = Chroma.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=embedding_model,\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "        \n",
    "        # Persist the database\n",
    "        vector_store.persist()\n",
    "        print(f\"Indexed {len(documents)} document chunks in Chroma DB\")\n",
    "        \n",
    "        return vector_store\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Chroma DB: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_existing_chroma_db(embedding_model, persist_directory: str = CHROMA_DB_DIR):\n",
    "    \"\"\"Load existing Chroma vector store.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        vector_store = Chroma(\n",
    "            persist_directory=persist_directory,\n",
    "            embedding_function=embedding_model\n",
    "        )\n",
    "        return vector_store\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading existing Chroma DB: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565965ab",
   "metadata": {},
   "source": [
    "# Openai Answer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abdfb5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_with_openai(question: str, retrieved_chunks: List[Document]) -> Dict[str, Any]:\n",
    "    \"\"\"Generate answer using OpenAI based on retrieved chunks.\"\"\"\n",
    "    \n",
    "    if not retrieved_chunks:\n",
    "        return {\"error\": \"No relevant chunks found\"}\n",
    "    \n",
    "    # Construct context from retrieved chunks\n",
    "    context_parts = []\n",
    "    for i, chunk in enumerate(retrieved_chunks):\n",
    "        source_file = chunk.metadata.get('source_file', 'Unknown')\n",
    "        context_parts.append(f\"[Source {i+1}: {source_file}]\\n{chunk.page_content}\\n\")\n",
    "    \n",
    "    context = \"\\n\".join(context_parts)\n",
    "    \n",
    "    # Create prompt for OpenAI\n",
    "    prompt = f\"\"\"You are an expert assistant helping with document analysis. Based on the provided context from various documents, please answer the user's question accurately and comprehensively.\n",
    "\n",
    "Context from documents:\n",
    "{context}\n",
    "\n",
    "User Question: {question}\n",
    "\n",
    "Please provide a detailed answer based on the context above. If the context doesn't contain enough information to fully answer the question, please mention what information is available and what might be missing. Always cite which source document(s) you're referencing in your answer.\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Updated for openai>=1.0.0\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that analyzes documents and provides accurate, well-sourced answers.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=1000,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"sources\": [chunk.metadata.get('source_file', 'Unknown') for chunk in retrieved_chunks],\n",
    "            \"num_chunks_used\": len(retrieved_chunks)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"OpenAI API error: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576d7767",
   "metadata": {},
   "source": [
    "# Main RAG Query Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d5f9b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag_system(question: str, vector_store=None, k: int = 3, use_openai: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"Main RAG query function that accepts any user question.\"\"\"\n",
    "    \n",
    "    if vector_store is None:\n",
    "        return {\"error\": \"Vector store not initialized\"}\n",
    "    \n",
    "    if not question.strip():\n",
    "        return {\"error\": \"Empty question provided\"}\n",
    "    \n",
    "    try:\n",
    "        # Retrieve relevant chunks using Chroma\n",
    "        results_with_scores = vector_store.similarity_search_with_score(question, k=k)\n",
    "        \n",
    "        if not results_with_scores:\n",
    "            return {\"error\": \"No relevant documents found\"}\n",
    "        \n",
    "        # Extract documents and scores\n",
    "        retrieved_docs = [doc for doc, score in results_with_scores]\n",
    "        similarity_scores = [1.0 - score for doc, score in results_with_scores]  # Convert distance to similarity\n",
    "        \n",
    "        # Generate answer using OpenAI if requested\n",
    "        if use_openai:\n",
    "            ai_response = generate_answer_with_openai(question, retrieved_docs)\n",
    "            if \"error\" in ai_response:\n",
    "                return ai_response\n",
    "            \n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"ai_answer\": ai_response[\"answer\"],\n",
    "                \"sources\": ai_response[\"sources\"],\n",
    "                \"similarity_scores\": similarity_scores,\n",
    "                \"retrieved_chunks\": [doc.page_content[:200] + \"...\" for doc in retrieved_docs],\n",
    "                \"num_chunks\": len(retrieved_docs)\n",
    "            }\n",
    "        else:\n",
    "            # Return raw chunks without OpenAI processing\n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"similarity_scores\": similarity_scores,\n",
    "                \"retrieved_chunks\": [doc.page_content for doc in retrieved_docs],\n",
    "                \"sources\": [doc.metadata.get('source_file', 'Unknown') for doc in retrieved_docs],\n",
    "                \"num_chunks\": len(retrieved_docs)\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Query error: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1ac1bd",
   "metadata": {},
   "source": [
    "# System Setup Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dad43aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_rag_system(chunk_config: str = \"default\", force_rebuild: bool = False):\n",
    "    \"\"\"Initialize the complete RAG system.\"\"\"\n",
    "    \n",
    "    print(\"üöÄ Initializing RAG System...\")\n",
    "    \n",
    "    # Setup embedding model\n",
    "    embedding_model = setup_embedding_model()\n",
    "    if embedding_model is None:\n",
    "        return None, None\n",
    "    \n",
    "    # Check if we should rebuild or load existing database\n",
    "    if force_rebuild or not os.path.exists(CHROMA_DB_DIR):\n",
    "        print(\"üìö Processing PDF files...\")\n",
    "        \n",
    "        # Find and process PDF files\n",
    "        pdf_data = find_and_process_pdfs(PDF_DIRECTORY)\n",
    "        \n",
    "        if not pdf_data:\n",
    "            print(\"‚ùå No PDF files found or processed\")\n",
    "            return None, None\n",
    "        \n",
    "        # Prepare documents with chunking\n",
    "        all_documents = []\n",
    "        chunk_size = CHUNK_CONFIGS[chunk_config][\"size\"]\n",
    "        chunk_overlap = CHUNK_CONFIGS[chunk_config][\"overlap\"]\n",
    "        \n",
    "        for pdf_info in pdf_data:\n",
    "            docs = prepare_documents_enhanced(\n",
    "                pdf_info[\"text\"], \n",
    "                pdf_info[\"filename\"], \n",
    "                chunk_size, \n",
    "                chunk_overlap\n",
    "            )\n",
    "            all_documents.extend(docs)\n",
    "        \n",
    "        print(f\"üìÑ Created {len(all_documents)} document chunks\")\n",
    "        \n",
    "        # Initialize Chroma database\n",
    "        vector_store = initialize_chroma_db(all_documents, embedding_model)\n",
    "        \n",
    "    else:\n",
    "        print(\"üìÅ Loading existing Chroma database...\")\n",
    "        vector_store = load_existing_chroma_db(embedding_model)\n",
    "    \n",
    "    if vector_store is None:\n",
    "        print(\"‚ùå Failed to initialize vector store\")\n",
    "        return None, None\n",
    "    \n",
    "    print(\"‚úÖ RAG System ready!\")\n",
    "    return vector_store, embedding_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d47a552",
   "metadata": {},
   "source": [
    "# Interactive Query Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c91d05a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question: str, detailed: bool = False):\n",
    "    \"\"\"Convenient function to ask questions and get formatted responses.\"\"\"\n",
    "    \n",
    "    if 'vector_store' not in globals() or vector_store is None:\n",
    "        print(\"‚ùå Vector store not initialized. Run setup_rag_system() first.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nü§î Question: {question}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    result = query_rag_system(question, vector_store, k=3, use_openai=True)\n",
    "    \n",
    "    if \"error\" in result:\n",
    "        print(f\"‚ùå Error: {result['error']}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ü§ñ AI Answer:\")\n",
    "    print(result['ai_answer'])\n",
    "    \n",
    "    if detailed:\n",
    "        print(f\"\\nüìä Details:\")\n",
    "        print(f\"  ‚Ä¢ Sources: {', '.join(set(result['sources']))}\")\n",
    "        print(f\"  ‚Ä¢ Chunks used: {result['num_chunks']}\")\n",
    "        print(f\"  ‚Ä¢ Similarity scores: {[f'{score:.3f}' for score in result['similarity_scores']]}\")\n",
    "        \n",
    "        print(f\"\\nüìÑ Retrieved chunks preview:\")\n",
    "        for i, chunk in enumerate(result['retrieved_chunks']):\n",
    "            print(f\"  Chunk {i+1}: {chunk}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460140cf",
   "metadata": {},
   "source": [
    "# Initialize the System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2746846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ RAG System Ready to Initialize!\n",
      "\n",
      "To get started, run:\n",
      "vector_store, embedding_model = setup_rag_system()\n",
      "\n",
      "Then ask questions with:\n",
      "ask_question(\"How are HOA board elections conducted?\")\n"
     ]
    }
   ],
   "source": [
    "print(\"üéØ RAG System Ready to Initialize!\")\n",
    "print(\"\\nTo get started, run:\")\n",
    "print(\"vector_store, embedding_model = setup_rag_system()\")\n",
    "print(\"\\nThen ask questions with:\")\n",
    "print('ask_question(\"How are HOA board elections conducted?\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4f1fc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing RAG System...\n",
      "üñ•Ô∏è Using device: cuda\n",
      "üìÅ Loading existing Chroma database...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\default.LAPTOP-4HP0OBME\\AppData\\Local\\Temp\\ipykernel_6124\\1666805180.py:30: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_store = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG System ready!\n"
     ]
    }
   ],
   "source": [
    " vector_store, embedding_model = setup_rag_system(chunk_config=\"default\", force_rebuild=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aa8f32",
   "metadata": {},
   "source": [
    "# Example Questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebeee9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§î Question: For Highland view How are HOA board elections conducted?\n",
      "------------------------------------------------------------\n",
      "‚ùå Error: OpenAI API error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    }
   ],
   "source": [
    "# Ask a question with a clean, formatted response\n",
    "ask_question(\"For Highland view How are HOA board elections conducted?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5eb31733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§î Question: What are the voting requirements for board meetings at Cardigan?\n",
      "------------------------------------------------------------\n",
      "‚ùå Error: OpenAI API error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    }
   ],
   "source": [
    "# Ask a question and see detailed metrics\n",
    "ask_question(\"What are the voting requirements for board meetings at Cardigan?\", detailed=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
